{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOxcBRes9TCmM01yBnaJ4Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamVedanta/Neural_network_from_scratch/blob/main/Neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7C4VLNrenAW"
      },
      "outputs": [],
      "source": [
        "#implementing neural network from scratch\n",
        "#Probelm - Digit classsification using the MNSIT dataset\n",
        "#It's a 28x28pixel grayscale images of handwritten digits, each of it is a pixel value between 0 and 255\n",
        "#0-> black, 255-> white\n",
        "#28x28=784 pixel -> [......]\n",
        "# take the transpose of the example to tell each column as an example\n",
        "#first layer- 784 nodes\n",
        "#second layer(hidden layer) - 10 nodes\n",
        "#output layer - 10 nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4iynzeEZfrMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three parts to running this network\n",
        "\n",
        "1. First part is **foward propagation**. Take an image and run it through the network. through the network you compute what the output would be\n",
        "\n",
        "Z1 is the unactivated first layer.\n",
        "Z1 = W x A + M\n",
        "\n",
        "Now put the activation function\n",
        "If you dont put the activation function, then each node will just be the linear combination of previous layer, this wont give any interesting output to study with.\n",
        "\n",
        "\n",
        "We use ReLu here.\n",
        "\n",
        "\n",
        "2. Backward propagation\n",
        "3. Update parameters\n"
      ],
      "metadata": {
        "id": "OqbApg4CvIGm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vVoB1960wdQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}